{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of deep learning\n",
    "\n",
    "**Deep Learning** (DL) is a class of Machine Learning methods that aims at learning feature hierarchies. DL is not  the solution but a useful set of tools for building A.I.\n",
    "\n",
    "We have lots of successful applications on the following contents, which can be viewed as a hierarchical structure.\n",
    "\n",
    "- Vision <br>\n",
    "  pixel -> edge -> texton -> super-pixel -> part -> object\n",
    "- Text <br>\n",
    "  character -> word -> NP/VP/... -> clause -> sentence -> story\n",
    "- Audio <br>\n",
    "  sample -> spectral band -> formant -> motif -> phone -> word\n",
    "\n",
    "The hierarchical models usually need to be deep.\n",
    "\n",
    "We will introduce two main deep learning methods and their implementations.\n",
    "\n",
    "- Convolutional neural network (CNN)\n",
    "- Recurrent Neural Network (RNN)\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A Neural Network\n",
    "\n",
    "<img src=\"figs/nn.png\" style=\"zoom:40%\">\n",
    "<div align=\"center\">*A fully connected neural network with 2 hidden layers.*</div>\n",
    "\n",
    "A neural network is constructed by an input layer, several hidden layers and an output layer. \n",
    "An [**activation fuction**](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks) is followed by each layer, generally there are three types -- sigmoid, tanh and ReLU.\n",
    "They introduce non-linearity, here we use the ReLU:\n",
    "\n",
    "$$f(x)=max(0, x)$$\n",
    "\n",
    "The ReLU layers accomplish as a local linear approximation. Multiple layers yeild exponential savings in number of parameters, through parameter sharing. It can easily obtain by using TensorFlow (TF): <br>\n",
    "> `tf.nn.relu(x)`\n",
    "\n",
    "Let's explicitly write expression of the computational process from input to output, which is called **Forward Propagation**.\n",
    "- $h_1 = max(0, W_1x + b_1)$\n",
    "- $h_2 = max(0, W_2h_1 + b_2)$\n",
    "- ...\n",
    "- $o = W_nh_{n-1} + b_n$\n",
    "\n",
    "Correspondingly, in TensorFlow, we use: <br>\n",
    "> `h_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)`\n",
    "\n",
    "In the following, we use **softmax** funtion to turn the outputs to the probabilities, which can be write as:\n",
    "$$p(c_j=1|x) = \\frac{e^{o_j}}{\\sum_{i=1}^{c}e^{o_i}}$$\n",
    "\n",
    "We now have the probability vector as the output. Since we already have the labelled vector $y$, we can use a [**Loss function**](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/) (or **cost** function) to measure the distance between these two vectors.\n",
    "Here we introduce a particular loss function called **Cross entropy**.\n",
    "$$L(x, y; \\theta) = - \\sum_j y_j \\text{log} p(c_j|x)$$\n",
    "The above two steps are combined into the same code in TF\n",
    "> `tf.nn.softmax_cross_entropy_with_logits(o, y)`\n",
    "\n",
    "To prevent overfitting, we introduce a regularization term (also called a weight decay term) into the loss function, which tends to decrease the magnitude of the weights.\n",
    "\n",
    "The training (learning) process mainly involve in minimizing the loss function, which use another important method, [**back propagation**](http://neuralnetworksanddeeplearning.com/chap2.html).\n",
    "Back propagation is about understanding how changing the weights and biases in a network changes the loss function.\n",
    "The back propagation is based on the chain rule,\n",
    "- $\\frac{\\partial L}{\\partial W_n} = \\frac{\\partial L}{\\partial o}\\frac{\\partial o}{\\partial W_n}$, $\\frac{\\partial L}{\\partial h_{n-1}} = \\frac{\\partial L}{\\partial o}\\frac{\\partial o}{\\partial h_{n-1}}$\n",
    "- ...\n",
    "- $\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial h_2}\\frac{\\partial h_2}{\\partial W_2}$, $\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_2}\\frac{\\partial h_2}{\\partial h_1}$\n",
    "- $\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial h_1}\\frac{\\partial h_1}{\\partial W_1}$\n",
    "\n",
    "Follow this procedure, we can compute $\\partial L/\\partial W_i$ and $\\partial L/\\partial b_i$.\n",
    "\n",
    "We use [**Stochastic Gradient Descent**](http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/) method for optimization, which updates the parameters $\\theta$ as\n",
    "$$\\theta = \\theta - \\alpha \\nabla_{\\theta} L(\\theta)$$\n",
    "$\\alpha$ is the learning rate. It can be easily implemented:\n",
    "> `tf.train.GradientDescentOptimizer(alpha).minimize(loss)`\n",
    "\n",
    "where alpha is the learning rate and loss is the loss function.\n",
    "\n",
    "The whole training process can be implemented by TF as following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "  W_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_neurons_1]))\n",
    "  b_1 = tf.Variable(tf.zeros([num_neurons_1]))\n",
    "  h_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "  W_2 = tf.Variable(tf.truncated_normal([num_neurons_1, num_neurons_2]))\n",
    "  b_2 = tf.Variable(tf.zeros([num_neurons_2]))  \n",
    "  h_2 = tf.matmul(h_1, W_2) + b_2\n",
    "    \n",
    "  W_3 = tf.Variable(tf.truncated_normal([num_neurons_2, num_labels]))\n",
    "  b_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  o = tf.matmul(h_2, W_3) + b_3\n",
    "\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(o, y) +\n",
    "    0.01*tf.nn.l2_loss(W_1) + 0.01*tf.nn.l2_loss(b_1) +\n",
    "    0.01*tf.nn.l2_loss(W_2) + 0.01*tf.nn.l2_loss(b_2) +    \n",
    "    0.01*tf.nn.l2_loss(W_3) + 0.01*tf.nn.l2_loss(b_3))\n",
    "\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convolutional neural network (CNN)\n",
    "\n",
    "A Convolutional Neural Network (CNN) is comprised of several convolutional layers and subsampling layers, followed by fully connected multilayer neural network. The CNNs have been successfully applied to analyzing visual imagery \n",
    "ï¿¼\n",
    "\n",
    "#### Convolutional layer\n",
    "The input to a convolutional layer is a $m \\times m \\times r$ image where $m$ is the height and width of the image and $r$ is the number of channels.\n",
    "\n",
    "The convolutional layer will have k patches (filters/kernels) of size $n \\times n \\times q$ where $n$ is smaller than the dimension of the image and $q$ is the depth of the patch and may vary for each patch. The size of the patch gives rise to the locally connected structure which are each convolved with the image to produce $k$ feature maps, $k = m-n+1$.\n",
    "\n",
    "#### Pooling layer\n",
    "We could compute the average (or max) value of a particular feature over a region of the image.\n",
    "We call it **pooling** operation, the main pooling methods are Max pooling, Average pooling ...\n",
    "\n",
    "Here we take a CNN with 2 convolutional layers (include pooling layer) followed by a neural network as an example.\n",
    "\n",
    "<img src=\"figs/conv.png\" style=\"zoom:50%\">\n",
    "<div align=\"center\">*Example of a convolutional nexwork with 2 convolutional layers and 1 hidden neural layer.*</div>\n",
    "\n",
    "The code for training by FT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "  W_1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "  b_1 = tf.Variable(tf.zeros([depth_1]))\n",
    "\n",
    "  W_2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "  b_2 = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  \n",
    "  W_3 = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  b_3 = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  W_4 = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "  b_4 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv_1 = tf.nn.conv2d(data, W_1, [1, 1, 1, 1], padding='SAME')\n",
    "    c_1 = tf.nn.relu(conv_1 + b_1)\n",
    "    pool_1 = tf.nn.max_pool(c_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    conv_2 = tf.nn.conv2d(pool_1, W_2, [1, 1, 1, 1], padding='SAME')\n",
    "    c_2 = tf.nn.relu(conv_2 + b_2)\n",
    "    pool_2 = tf.nn.max_pool(c_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    shape = pool_2.get_shape().as_list()\n",
    "    reshape = tf.reshape(poo1_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    h = tf.nn.relu(tf.matmul(reshape, W_3) + b_3)\n",
    "    dropout = tf.nn.dropout(h, 0.5)\n",
    "    \n",
    "    o = tf.matmul(dropout, W_4) + b_4\n",
    "    return o\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(x)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y)+\n",
    "    0.01*tf.nn.l2_loss(W_1) + 0.01*tf.nn.l2_loss(b_1) +\n",
    "    0.01*tf.nn.l2_loss(W_2) + 0.01*tf.nn.l2_loss(b_2) +    \n",
    "    0.01*tf.nn.l2_loss(W_3) + 0.01*tf.nn.l2_loss(b_3) +\n",
    "    0.01*tf.nn.l2_loss(W_4) + 0.01*tf.nn.l2_loss(b_4))\n",
    "\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(alpha).minimize(loss)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Recurrent neural network (RNN)\n",
    "\n",
    "RNNs allow us to operate over sequential data (text, speech, handwritings, etc.), sequences as both the input and output.\n",
    "RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. We can imagine that the networks have a âmemoryâ which stores information about what has been calculated so far. \n",
    "\n",
    "#### Simple recurrent network\n",
    "<img src=\"figs/srn.png\" style=\"zoom:40%\">\n",
    "<div align=\"center\">*Example of a simple recurrent network.*</div>\n",
    "\n",
    "The unit of a simple recurrent network can be expressed as\n",
    "- $h_t = \\sigma_h(W_{xh}x_t+W_{hh}h_{t-1}+b_h)$\n",
    "- $o_t = \\sigma_o(W_{ho}h_t + b_y)$\n",
    "\n",
    "in which $x_t$ is the $t$-th input vector, $h_t$ is the hidden layer and $o_t$ is the output vector; $W_{xh},W_{hh},W_{ho}$ are weight parameter matrices that are shared across all steps, $\\sigma$ is the activation function.\n",
    "\n",
    "#### Long short-term memory (LSTM)\n",
    "A LSTM unit\n",
    "<img src=\"figs/lstm.png\" style=\"zoom:40%\">\n",
    "<div align=\"center\">*A LSTM unit.*</div>\n",
    "\n",
    "Two outputs from the LSTM unit\n",
    "- $h_t = o_t * tanh(c_t)$\n",
    "- $c_t = f_t * c_{t-1} + i_t*\\tilde{c}_t$\n",
    "\n",
    "in which, we have three gates and a memory cell\n",
    "- Input gate: $i_t = \\sigma(W_ix_t + U_ih_{t-1} + b_i)$\n",
    "- Forget gate: $f_t = \\sigma(W_fx_t + U_fh_{t-1} + b_f)$\n",
    "- Output gate: $o_t = \\sigma(W_ox_t + U_oh_{t-1} + b_o)$\n",
    "- Memory cell: $\\tilde{c}_t = tanh(W_cx_t + U_ch_{t-1} + b_c)$\n",
    "\n",
    "We can view the LSTM unit as a momory cell, which can store informations through the operation gates write, read and erase.\n",
    "\n",
    "The TF code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(x, h, c):\n",
    "# gates\n",
    "    i= tf.sigmoid(tf.matmul(x, W_i) + tf.matmul(h, U_i) + b_i)\n",
    "    f = tf.sigmoid(tf.matmul(x, W_f) + tf.matmul(h, U_f) + b_f)\n",
    "    o = tf.sigmoid(tf.matmul(x, W_o) + tf.matmul(h, U_o) + b_o)\n",
    "# memory cell\n",
    "    update = tf.tanh(tf.matmul(x, W_c) + tf.matmul(h, U_c) + b_c)\n",
    "# outputs\n",
    "    state = f * c + i * update\n",
    "    output_h = o * tf.tanh(state)\n",
    "    return output_h, state\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
