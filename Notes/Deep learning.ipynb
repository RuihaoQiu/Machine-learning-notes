{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of deep learning\n",
    "\n",
    "**Deep Learning** (DL) is a class of Machine Learning methods that aims at learning feature hierarchies. DL is not  the solution but a useful set of tools for building A.I.\n",
    "\n",
    "We have lots of successful applications on the following contents, which can be viewed as a hierarchical structure.\n",
    "\n",
    "- Vision \n",
    "  pixel -> edge -> texton -> super-pixel -> part -> object\n",
    "- Text \n",
    "  character -> word -> NP/VP/... -> clause -> sentence -> story\n",
    "- Audio\n",
    "  sample -> spectral band -> formant -> motif -> phone -> word\n",
    "\n",
    "The hierarchical models usually need to be deep.\n",
    "\n",
    "We will introduce two main deep learning methods and their implementations.\n",
    "\n",
    "- Convolutional neural network (CNN)\n",
    "- Recurrent Neural Network (RNN)\t\t\n",
    "\n",
    "### 1. A Neural Network\n",
    "\n",
    "![](figs/nn.png)\n",
    "<div align=\"center\">*Example of a 2 hidden layer neural network.*</div>\n",
    "\n",
    "\n",
    "A neural network is constructed by an input layer, several hidden layers and an output layer. \n",
    "An [**activation fuction**](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks) is followed by each layer, generally there are three types -- sigmoid, tanh and ReLU, here we use ReLU.\n",
    "The ReLU introduces non-linearity: \n",
    "\n",
    "$$f(x)=max(0, x)$$\n",
    "\n",
    "The ReLU layers accomplish as a local linear approximation. Multiple layers yeild exponential savings in number of parameters, through parameter sharing.\n",
    "\n",
    "Let's explicitly write expression of the computational process from input to output, which is called **Forward Propagation**.\n",
    "- $h_1 = max(0, W_1x + b_1)$\n",
    "- $h_2 = max(0, W_2h_1 + b_2)$\n",
    "- ...\n",
    "- $o = max(0, W_nh_{n-1} + b_n)$\n",
    "\n",
    "In the following, we use **softmax** funtion to turn the outputs to the probabilities, which can be write as:\n",
    "$$p(c_j=1|x) = \\frac{e^{o_j}}{\\sum_{i=1}^{c}e^{o_i}}$$\n",
    "\n",
    "We now have the probability vector as the output. Since we already have the labelled vector $y$, we can use a **Loss** function (or **cost** function) to measure the distance between these two vectors\n",
    "$$L(x, y; \\theta) = - \\sum_j y_j \\text{log} p(c_j|x)$$\n",
    "This loss function is also called **Cross entropy**.\n",
    "\n",
    "The training (learning) process mainly involve in minimizing the loss function, which use another important method, [**back propagation**](http://neuralnetworksanddeeplearning.com/chap2.html).\n",
    "Back propagation is about understanding how changing the weights and biases in a network changes the loss function.\n",
    "The back propagation is based on the chain rule,\n",
    "- $\\frac{\\partial L}{\\partial W_n} = \\frac{\\partial L}{\\partial o}\\frac{\\partial o}{\\partial W_n}$, $\\frac{\\partial L}{\\partial h_{n-1}} = \\frac{\\partial L}{\\partial o}\\frac{\\partial o}{\\partial h_{n-1}}$\n",
    "- ...\n",
    "- $\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial h_2}\\frac{\\partial h_2}{\\partial W_2}$, $\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_2}\\frac{\\partial h_2}{\\partial h_1}$\n",
    "- $\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial h_1}\\frac{\\partial h_1}{\\partial W_1}$\n",
    "\n",
    "Follow this procedure, we can compute $\\partial L/\\partial W_i$ and $\\partial L/\\partial b_i$.\n",
    "\n",
    "We use [**Stochastic Gradient Descent**](http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/) method for optimization, which updates the parameters $\\theta$ as\n",
    "$$\\theta = \\theta - \\alpha \\nabla_{\\theta} L(\\theta)$$\n",
    "$\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
